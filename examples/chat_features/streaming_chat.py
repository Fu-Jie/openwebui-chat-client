#!/usr/bin/env python3
"""
Streaming chat example for OpenWebUI Chat Client.

This example demonstrates real-time streaming chat functionality, where responses
are received and displayed incrementally as they are generated.

Features demonstrated:
- Real-time streaming chat responses
- Streaming with follow-up suggestions
- Streaming with image inputs (multimodal)
- Error handling in streaming scenarios

Requirements:
- Environment variable: OUI_BASE_URL
- Environment variable: OUI_AUTH_TOKEN

Usage:
    python examples/chat_features/streaming_chat.py
"""

import logging
import os
import sys
from typing import Optional, Generator

# Add the parent directory to path to import the client and utils
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from openwebui_chat_client import OpenWebUIClient
from dotenv import load_dotenv
from utils.file_helpers import TestFileManager

# Load environment variables
load_dotenv()

# Configuration
BASE_URL = os.getenv("OUI_BASE_URL", "http://localhost:3000")
AUTH_TOKEN = os.getenv("OUI_AUTH_TOKEN")
DEFAULT_MODEL = os.getenv("OUI_DEFAULT_MODEL", "gpt-4.1")
MULTIMODAL_MODEL = os.getenv("OUI_MULTIMODAL_MODEL", DEFAULT_MODEL)

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def basic_streaming_example(client: OpenWebUIClient) -> None:
    """Demonstrate basic streaming chat functionality."""
    logger.info("üì° Basic Streaming Chat Example")
    logger.info("=" * 40)
    
    question = "Write a short poem about artificial intelligence"
    chat_title = "AI Poetry Streaming"
    
    logger.info(f"üéØ Question: {question}")
    logger.info("üí≠ Streaming response:")
    
    print("\n" + "=" * 50)
    print("ü§ñ AI Response (Streaming):")
    print("=" * 50)
    
    try:
        # Stream the response
        for chunk in client.stream_chat(
            question=question,
            chat_title=chat_title,
            model_id=DEFAULT_MODEL
        ):
            print(chunk, end="", flush=True)
        
        print("\n" + "=" * 50)
        logger.info("‚úÖ Streaming completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Streaming failed: {e}")


def streaming_with_follow_up_example(client: OpenWebUIClient) -> None:
    """Demonstrate streaming with follow-up suggestions."""
    logger.info("\nüîÑ Streaming with Follow-up Suggestions")
    logger.info("=" * 50)
    
    question = "Explain the concept of machine learning"
    chat_title = "ML Explanation Streaming"
    
    logger.info(f"üéØ Question: {question}")
    logger.info("üí≠ Streaming response with follow-up suggestions:")
    
    print("\n" + "=" * 50)
    print("ü§ñ AI Response (Streaming with Follow-up):")
    print("=" * 50)
    
    try:
        # Stream with follow-up enabled
        stream_gen = client.stream_chat(
            question=question,
            chat_title=chat_title,
            enable_follow_up=True
        )
        
        # Process streaming chunks
        for chunk in stream_gen:
            print(chunk, end="", flush=True)
        
        # Get the return values when streaming completes
        try:
            final_content, sources, follow_ups = stream_gen.value
            print("\n" + "=" * 50)
            
            if follow_ups:
                logger.info("ü§î Follow-up suggestions received:")
                for i, suggestion in enumerate(follow_ups, 1):
                    print(f"  {i}. {suggestion}")
            else:
                logger.info("‚ÑπÔ∏è No follow-up suggestions generated")
                
        except AttributeError:
            # stream_gen.value might not be available in all versions
            logger.info("‚ÑπÔ∏è Follow-up suggestions not available in this version")
        
        logger.info("‚úÖ Streaming with follow-up completed")
        
    except Exception as e:
        logger.error(f"‚ùå Streaming with follow-up failed: {e}")


def streaming_with_image_example(client: OpenWebUIClient) -> None:
    """Demonstrate streaming chat with image input."""
    logger.info("\nüñºÔ∏è Streaming Chat with Image Input")
    logger.info("=" * 45)
    
    # Create a test image
    with TestFileManager() as file_manager:
        image_path = file_manager.create_test_file(
            "streaming_test.png", 
            "Streaming Chat Test Image",
            "image"
        )
        
        if not image_path:
            logger.warning("‚ö†Ô∏è Could not create test image - skipping this example")
            return
        
        question = "What do you see in this image? Describe it in detail."
        chat_title = "Image Description Streaming"
        
        logger.info(f"üéØ Question: {question}")
        logger.info(f"üñºÔ∏è Image: {image_path}")
        logger.info("üí≠ Streaming response:")
        
        print("\n" + "=" * 50)
        print("ü§ñ AI Response (Streaming with Image):")
        print("=" * 50)
        
        try:
            # Stream with image input
            for chunk in client.stream_chat(
                question=question,
                chat_title=chat_title,
                image_paths=[image_path],
                model_id=MULTIMODAL_MODEL
            ):
                print(chunk, end="", flush=True)
            
            print("\n" + "=" * 50)
            logger.info("‚úÖ Streaming with image completed successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Streaming with image failed: {e}")


def streaming_error_handling_example(client: OpenWebUIClient) -> None:
    """Demonstrate error handling in streaming scenarios."""
    logger.info("\n‚ö†Ô∏è Streaming Error Handling Example")
    logger.info("=" * 45)
    
    # Test with an intentionally problematic request
    question = "x" * 10000  # Very long question that might cause issues
    chat_title = "Error Handling Test"
    
    logger.info("üß™ Testing error handling with problematic input")
    
    try:
        # Attempt to stream with problematic input
        response_chunks = []
        for chunk in client.stream_chat(
            question=question,
            chat_title=chat_title,
            model_id="non-existent-model"  # This should cause an error
        ):
            response_chunks.append(chunk)
            if len(response_chunks) > 10:  # Limit output for demonstration
                break
        
        if response_chunks:
            logger.info("ü§î Unexpectedly received response chunks")
        else:
            logger.info("‚ÑπÔ∏è No response chunks received")
            
    except Exception as e:
        logger.info(f"‚úÖ Error handled correctly: {e}")
    
    # Test with a valid but simple request to show recovery
    logger.info("üîÑ Testing recovery with valid request...")
    
    try:
        recovery_chunks = []
        for chunk in client.stream_chat(
            question="Hello, are you working?",
            chat_title="Recovery Test",
            model_id=DEFAULT_MODEL
        ):
            recovery_chunks.append(chunk)
            if len(recovery_chunks) > 5:  # Just get a few chunks
                break
        
        if recovery_chunks:
            logger.info(f"‚úÖ Successfully recovered - received {len(recovery_chunks)} chunks")
        else:
            logger.warning("‚ö†Ô∏è Recovery failed - no chunks received")
            
    except Exception as e:
        logger.error(f"‚ùå Recovery failed: {e}")


def main() -> None:
    """Main function demonstrating streaming chat functionality."""
    logger.info("üöÄ OpenWebUI Chat Client - Streaming Chat Examples")
    logger.info("=" * 65)
    
    # Validation
    if not AUTH_TOKEN:
        logger.error("‚ùå OUI_AUTH_TOKEN environment variable not set")
        logger.error("Please set your OpenWebUI API token:")
        logger.error("  export OUI_AUTH_TOKEN='your_token_here'")
        return
    
    # Client initialization
    try:
        client = OpenWebUIClient(BASE_URL, AUTH_TOKEN, DEFAULT_MODEL)
        logger.info("‚úÖ Client initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize client: {e}")
        return
    
    # Run streaming examples
    try:
        basic_streaming_example(client)
        streaming_with_follow_up_example(client)
        streaming_with_image_example(client)
        streaming_error_handling_example(client)
        
        logger.info("\nüéâ Streaming chat examples completed successfully!")
        logger.info("üí° Next steps:")
        logger.info("   - Try: python examples/chat_features/parallel_chat.py")
        logger.info("   - Try: python examples/advanced_features/real_time_streaming.py")
        
    except Exception as e:
        logger.error(f"‚ùå Streaming chat examples failed: {e}")


if __name__ == "__main__":
    main()