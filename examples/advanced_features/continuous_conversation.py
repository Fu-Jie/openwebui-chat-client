#!/usr/bin/env python3
"""
Continuous Conversation Example for OpenWebUI Chat Client.

This example demonstrates the new continuous conversation features that allow
automated multi-turn conversations using follow-up suggestions.

Features demonstrated:
- Continuous single-model conversations
- Continuous parallel-model conversations  
- Continuous streaming conversations
- Automatic follow-up question selection
- Generic fallback questions when no follow-ups available
- Error handling and conversation management

Requirements:
- Environment variable: OUI_BASE_URL
- Environment variable: OUI_AUTH_TOKEN
- Environment variable: OUI_DEFAULT_MODEL (optional)
- Environment variable: OUI_PARALLEL_MODELS (optional)

Usage:
    python examples/advanced_features/continuous_conversation.py
"""

import logging
import os
import sys
from typing import Optional, List

# Add the parent directory to path to import the client and utils
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from openwebui_chat_client import OpenWebUIClient
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration
BASE_URL = os.getenv("OUI_BASE_URL", "http://localhost:3000")
AUTH_TOKEN = os.getenv("OUI_AUTH_TOKEN")
DEFAULT_MODEL = os.getenv("OUI_DEFAULT_MODEL", "gpt-4.1")
PARALLEL_MODELS = os.getenv("OUI_PARALLEL_MODELS", "gpt-4.1,gemini-2.5-flash").split(",")

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def continuous_single_model_example(client: OpenWebUIClient) -> None:
    """Demonstrate continuous conversation with a single model."""
    logger.info("ğŸ”„ Continuous Single-Model Conversation Example")
    logger.info("=" * 60)
    
    initial_question = "Explain the concept of artificial intelligence"
    num_questions = 3
    chat_title = "AI Deep Dive - Continuous"
    
    logger.info(f"ğŸ¯ Starting conversation: {initial_question}")
    logger.info(f"ğŸ“Š Target rounds: {num_questions}")
    
    try:
        result = client.continuous_chat(
            initial_question=initial_question,
            num_questions=num_questions,
            chat_title=chat_title,
            model_id=DEFAULT_MODEL,
            folder_name="Continuous Conversations",
            tags=["ai", "continuous", "deep-dive"],
            enable_auto_tagging=True,
            enable_auto_titling=True
        )
        
        if result:
            logger.info(f"âœ… Conversation completed: {result['total_rounds']} rounds")
            logger.info(f"ğŸ’¬ Chat ID: {result['chat_id']}")
            
            print("\n" + "=" * 60)
            print("ğŸ“š CONVERSATION SUMMARY")
            print("=" * 60)
            
            for round_data in result['conversation_history']:
                print(f"\nğŸ”¹ Round {round_data['round']}:")
                print(f"â“ Question: {round_data['question']}")
                print(f"ğŸ¤– Response: {round_data['response'][:200]}...")
                
                if 'follow_ups' in round_data:
                    print(f"ğŸ¤” Follow-ups suggested: {len(round_data['follow_ups'])}")
                    for i, follow_up in enumerate(round_data['follow_ups'][:3], 1):
                        print(f"   {i}. {follow_up}")
            
            if 'suggested_tags' in result:
                print(f"\nğŸ·ï¸ Auto-generated tags: {result['suggested_tags']}")
            if 'suggested_title' in result:
                print(f"ğŸ“ Auto-generated title: {result['suggested_title']}")
                
        else:
            logger.error("âŒ Continuous conversation failed")
            
    except Exception as e:
        logger.error(f"âŒ Continuous single-model conversation failed: {e}")


def continuous_parallel_model_example(client: OpenWebUIClient) -> None:
    """Demonstrate continuous conversation with multiple models in parallel."""
    logger.info("\nğŸ”„ Continuous Parallel-Model Conversation Example")
    logger.info("=" * 60)
    
    initial_question = "What are the most significant challenges in modern web development?"
    num_questions = 2
    chat_title = "Web Dev Challenges - Multi-Model Analysis"
    
    logger.info(f"ğŸ¯ Starting parallel conversation: {initial_question}")
    logger.info(f"ğŸ¤– Models: {PARALLEL_MODELS}")
    logger.info(f"ğŸ“Š Target rounds: {num_questions}")
    
    try:
        result = client.continuous_parallel_chat(
            initial_question=initial_question,
            num_questions=num_questions,
            chat_title=chat_title,
            model_ids=PARALLEL_MODELS,
            folder_name="Continuous Conversations",
            tags=["web-dev", "parallel", "analysis"],
        )
        
        if result:
            logger.info(f"âœ… Parallel conversation completed: {result['total_rounds']} rounds")
            logger.info(f"ğŸ’¬ Chat ID: {result['chat_id']}")
            
            print("\n" + "=" * 60)
            print("ğŸŒ PARALLEL CONVERSATION SUMMARY")
            print("=" * 60)
            
            for round_data in result['conversation_history']:
                print(f"\nğŸ”¹ Round {round_data['round']}:")
                print(f"â“ Question: {round_data['question']}")
                
                for model_id, response_data in round_data['responses'].items():
                    print(f"\nğŸ¤– {model_id}:")
                    print(f"   {response_data.get('content', 'No response')[:150]}...")
                
                if 'follow_ups' in round_data:
                    print(f"\nğŸ¤” Combined follow-ups: {len(round_data['follow_ups'])}")
                    for i, follow_up in enumerate(round_data['follow_ups'][:3], 1):
                        print(f"   {i}. {follow_up}")
                        
        else:
            logger.error("âŒ Continuous parallel conversation failed")
            
    except Exception as e:
        logger.error(f"âŒ Continuous parallel conversation failed: {e}")


def continuous_streaming_example(client: OpenWebUIClient) -> None:
    """Demonstrate continuous conversation with streaming responses."""
    logger.info("\nğŸ”„ Continuous Streaming Conversation Example")
    logger.info("=" * 60)
    
    initial_question = "Explain quantum computing in simple terms"
    num_questions = 2
    chat_title = "Quantum Computing - Streaming Discovery"
    
    logger.info(f"ğŸ¯ Starting streaming conversation: {initial_question}")
    logger.info(f"ğŸ“Š Target rounds: {num_questions}")
    
    try:
        print("\n" + "=" * 60)
        print("ğŸ“¡ STREAMING CONVERSATION")
        print("=" * 60)
        
        final_result = None
        current_round = 0
        
        for chunk in client.continuous_stream_chat(
            initial_question=initial_question,
            num_questions=num_questions,
            chat_title=chat_title,
            model_id=DEFAULT_MODEL,
            folder_name="Continuous Conversations",
            tags=["quantum", "streaming", "educational"]
        ):
            chunk_type = chunk.get("type")
            
            if chunk_type == "round_start":
                current_round = chunk["round"]
                print(f"\nğŸ”¹ Round {current_round} Starting...")
                print(f"â“ Question: {chunk['question']}")
                print("ğŸ¤– Response: ", end="", flush=True)
                
            elif chunk_type == "round_complete":
                print(f"\nâœ… Round {chunk['round']} completed")
                if chunk.get("follow_ups"):
                    print(f"ğŸ¤” Follow-ups available: {len(chunk['follow_ups'])}")
                    for i, follow_up in enumerate(chunk['follow_ups'][:2], 1):
                        print(f"   {i}. {follow_up}")
                        
            elif chunk_type == "conversation_complete":
                final_result = chunk["summary"]
                print(f"\nğŸ‰ Streaming conversation completed!")
                
            elif chunk_type == "round_error":
                print(f"\nâŒ Error in round {chunk['round']}: {chunk['error']}")
        
        if final_result:
            logger.info(f"âœ… Streaming conversation summary:")
            logger.info(f"   Total rounds: {final_result['total_rounds']}")
            logger.info(f"   Chat ID: {final_result['chat_id']}")
        else:
            logger.error("âŒ Streaming conversation failed to complete")
            
    except Exception as e:
        logger.error(f"âŒ Continuous streaming conversation failed: {e}")


def error_handling_example(client: OpenWebUIClient) -> None:
    """Demonstrate error handling in continuous conversations."""
    logger.info("\nâš ï¸ Error Handling Example")
    logger.info("=" * 40)
    
    # Test with invalid parameters
    logger.info("ğŸ§ª Testing invalid parameters...")
    
    # Test with zero questions
    result = client.continuous_chat(
        initial_question="Test",
        num_questions=0,  # Invalid
        chat_title="Error Test"
    )
    logger.info(f"Zero questions result: {result}")
    
    # Test parallel chat with empty model list
    result = client.continuous_parallel_chat(
        initial_question="Test", 
        num_questions=1,
        chat_title="Error Test",
        model_ids=[]  # Invalid
    )
    logger.info(f"Empty models result: {result}")
    
    logger.info("âœ… Error handling examples completed")


def main() -> None:
    """Main function demonstrating continuous conversation features."""
    logger.info("ğŸš€ OpenWebUI Chat Client - Continuous Conversation Examples")
    logger.info("=" * 80)
    
    # Validation
    if not AUTH_TOKEN:
        logger.error("âŒ OUI_AUTH_TOKEN environment variable not set")
        logger.error("Please set your OpenWebUI API token:")
        logger.error("  export OUI_AUTH_TOKEN='your_token_here'")
        return
    
    # Client initialization
    try:
        client = OpenWebUIClient(BASE_URL, AUTH_TOKEN, DEFAULT_MODEL)
        logger.info("âœ… Client initialized successfully")
        logger.info(f"ğŸ”— Base URL: {BASE_URL}")
        logger.info(f"ğŸ¤– Default model: {DEFAULT_MODEL}")
        logger.info(f"ğŸ”„ Parallel models: {PARALLEL_MODELS}")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize client: {e}")
        return
    
    # Run examples
    try:
        continuous_single_model_example(client)
        continuous_parallel_model_example(client)
        continuous_streaming_example(client)
        error_handling_example(client)
        
        logger.info("\nğŸ‰ All continuous conversation examples completed successfully!")
        logger.info("ğŸ’¡ Key features demonstrated:")
        logger.info("   âœ“ Automatic follow-up question generation and selection")
        logger.info("   âœ“ Single, parallel, and streaming conversation modes")
        logger.info("   âœ“ Generic fallback questions when follow-ups unavailable")
        logger.info("   âœ“ Conversation organization with folders and tags")
        logger.info("   âœ“ Auto-tagging and auto-titling capabilities")
        logger.info("   âœ“ Comprehensive error handling")
        
        logger.info("\nğŸ“š Next steps:")
        logger.info("   - Check your OpenWebUI interface to see the created conversations")
        logger.info("   - Try: python examples/chat_features/streaming_chat.py")
        logger.info("   - Try: python examples/rag_knowledge/file_rag.py")
        
    except Exception as e:
        logger.error(f"âŒ Continuous conversation examples failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()